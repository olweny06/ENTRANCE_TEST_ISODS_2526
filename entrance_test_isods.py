# -*- coding: utf-8 -*-
"""Entrance test ISODS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QoYOC00YxhTPl5H2UlFQ7DN1O7bztMN0
"""

import os
import requests
import zipfile
from google.colab import drive
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Connect to Google Drive
drive.mount('/content/drive')

# Paths to additional folders such as vbpl, property, history, related, pdf
base_dir = '/content/drive/My Drive/BoPhapDienDienTu'
demuc_path = os.path.join(base_dir, 'demuc')
vbpl_path = os.path.join(base_dir, 'vbpl')
property_path = os.path.join(base_dir, 'property')
history_path = os.path.join(base_dir, 'history')
related_path = os.path.join(base_dir, 'related')
pdf_path = os.path.join(base_dir, 'pdf')
# Check if these folders exist yet
os.makedirs(base_dir, exist_ok=True)
os.makedirs(property_path, exist_ok=True)
os.makedirs(history_path, exist_ok=True)
os.makedirs(related_path, exist_ok=True)
os.makedirs(pdf_path, exist_ok=True)

# Dowload ZIP file and save as BoPhapDienDienTu to base_dir
zip_url = 'https://phapdien.moj.gov.vn/TraCuuPhapDien/Files/BoPhapDienDienTu.zip'
zip_file_path = os.path.join(base_dir, 'BoPhapDienDienTu.zip')
response = requests.get(zip_url)
with open(zip_file_path, 'wb') as file:
    file.write(response.content)
print("Zip file had been downloaded")

# Extract ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(base_dir)
print("Zip file had been extracted")

# Create additional folders
os.makedirs(os.path.join(base_dir, 'vbpl'), exist_ok=True)
os.makedirs(os.path.join(base_dir, 'property'), exist_ok=True)
os.makedirs(os.path.join(base_dir, 'history'), exist_ok=True)

os.makedirs(os.path.join(base_dir, 'related'), exist_ok=True)
os.makedirs(os.path.join(base_dir, 'pdf'), exist_ok=True)
print("Folders had been created")

import asyncio
import aiohttp
download_paths = {
    "full": vbpl_path,
    "p": property_path,
    "h": history_path,
    "r": related_path,
    "pdf": pdf_path
}

async def download_and_save(session, url, item_id, prefix):
    """Asynchronously download content and save it to a file.
        Args: 
            session : used for making requests
            url : the URL to download the content from 
            item_id 
            prefix (str) : the prefix use for the filename based on the type of content 
            
    """
    filename = f"{prefix}_{item_id}.html" if prefix != "pdf" else f"{prefix}_{item_id}.pdf"
    filepath = os.path.join(download_paths[prefix], filename)

    try:
        async with session.get(url) as response:
          if response.status ==200:
            content = await response.text()
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(content)
            print(f"Saved: {filename}")
    except Exception as e:
        print(f"Failed: {url} - {e}")

async def process_file(file):
  '''
  Handle html files and return paths containing ItemID
  Args:
    file (str):  HTML file needs to be handled
  Returns:
    lists: List of tuples containing paths and ItemID
  '''
  if not file.endswith(".html"):
      return []
  if file.endswith('.html'):
    full_path = os.path.join(demuc_path, file)
    with open(full_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")

    links = soup.find_all("a", href=True)
    extracted_links = []
    for link in links:
      href = link["href"]
      if "ItemID" in href:
        try:

          item_id = href.split("ItemID=")[1].split("#")[0]
          full_url = urljoin("https://vbpl.vn", href)
          extracted_links.append((full_url, item_id))
        except IndexError:
          print(f"Warning: Could not extract ItemID from {href}")
  return extracted_links


async def process_all_files():
    """Process all files in the directory and download documents concurrently.
        This function collects all links from HTML files and creates download tasks 
        for each link, running them concurrently to improve efficiency.
    """
    all_links = []

    # Collect all links first
    for file in os.listdir(demuc_path):
        all_links.extend(await process_file(file))  # Assuming process_file() exists

    # Create download tasks
    async with aiohttp.ClientSession() as session:
        tasks = []
        for full_url, item_id in all_links:
            tasks.extend([
                download_and_save(session, full_url, item_id, "full"),
                download_and_save(session, f"https://vbpl.vn/tw/Pages/vbpq-thuoctinh.aspx?dvid=13&ItemID={item_id}&Keyword=", item_id, "p"),
                download_and_save(session, f"https://vbpl.vn/tw/Pages/vbpq-lichsu.aspx?dvid=13&ItemID={item_id}&Keyword=", item_id, "h"),
                download_and_save(session, f"https://vbpl.vn/TW/Pages/vbpq-vanbanlienquan.aspx?ItemID={item_id}&Keyword=", item_id, "r"),
                download_and_save(session, f"https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID={item_id}", item_id, "pdf"),
            ])

        # Run all tasks concurrently
        results = await asyncio.gather(*tasks)
        print(f"Downloaded {len(results)} files")

# Run the async function
try:
  loop = asyncio.get_running_loop()
except RuntimeError:
  loop = asyncio.new_event_loop()
  asyncio.set_event_loop(loop)

#loop.run_until_complete(process_all_files())
await process_all_files()

# Conver all the files processed in the each additional folders to CSV files

import pandas as pd

folders = [ 'vbpl', 'property', 'related', 'history', 'pdf']
csv_folder = os.path.join(base_dir, 'csv_files')
os.makedirs(csv_folder, exist_ok=True)

# Function to convert HTML files to CSV
def convert_html_to_csv(folder):
  """
    Convert HTML files in the specified folder to CSV format 
    Args: 
      folder (str) : The folder containing HTML files to convert
    
  """

  for file in os.listdir(folder):
    if file.endswith('.html'):
      full_path = os.path.join(folder, file)
      with open(full_path, 'r', encoding='utf-8') as f:
        content = f.read()
        soup = BeautifulSoup(content, 'html.parser')

        title = soup.title.string if soup.title else 'No Title'
        body = soup.get_text()

        # Create a DataFrame
        df = pd.DataFrame({
            'Title': [title],
            'Content': [body],
            'File Path': [full_path]
        })

        # Save the DataFrame to a CSV file
        csv_file_name = f"{os.path.splitext(file)[0]}.csv"  # Change the extension to .csv
        csv_file_path = os.path.join(csv_folder, csv_file_name)
        df.to_csv(csv_file_path, index=False)

        print(f"Converted {full_path} to {csv_file_path}")

# Convert files in each folder to CSV
for folder in folders:
  folder_path = os.path.join(base_dir, folder)
  convert_html_to_csv(folder_path)

print("All files have been converted to CSV format.")

#pip install langchain chromadb transformers

# pip install -U langchain-community

import os
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document


# Define base directory where the HTML files are stored
base_dir = '/content/drive/My Drive/BoPhapDienDienTu/vbpl'

# Create embeddings
embeddings = HuggingFaceEmbeddings(model_name = "bkai-foundation-models/vietnamese-bi-encoder")

# Create a list to store the documents
documents = []

# Define a text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=20)

# Reall all HTML files in the specified directory
for file in os.listdir(base_dir):
    if file.endswith('.html') and file.startswith('full_'):
        full_path = os.path.join(base_dir, file)

        with open(full_path, 'r', encoding='utf-8') as f:
            content = f.read()
        # Check if content is empty
        if not content.strip():
            print(f"Warning: Empty content in file: {full_path}")
            continue  # Skip empty files

        # Split content into chunks
        chunks = text_splitter.split_text(content)

        # Create document chunks with metadata
        for chunk in chunks:
            documents.append(Document(page_content=chunk, metadata={"file_path": full_path}))

print(f"Total number of documents: {len(documents)}")

# Create a vector database
vector_store = Chroma.from_documents(documents, embeddings)

print("The vector database has been created.")

persist_directory = '/content/drive/My Drive/BoPhapDienDienTu/vector_store'
vector_store.persist()

# Import necessary libraries
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from transformers import pipeline

# Initialize the language model for answering queries
llm_pipeline = pipeline("text2text-generation",
    model="VietAI/vit5-base",
    max_length=256,  # Limit output length
    do_sample=True,  # Disable random sampling
    temperature=0.1,  # Reduce randomness
    top_k=50,  # Consider only top 50 tokens

)
llm = HuggingFacePipeline(pipeline=llm_pipeline)

# Load the persisted vector store
persist_directory = '/content/drive/My Drive/BoPhapDienDienTu/vector_store'
vector_store = Chroma(persist_directory=persist_directory,
                      embedding_function=HuggingFaceEmbeddings(model_name = "bkai-foundation-models/vietnamese-bi-encoder"))
# Create a retriever
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# Define search function
def search_query(query, k=5):
    """Performs semantic search on the vector database and returns top-k document paths containing the query."""
    retriever.search_kwargs["k"] = k
    docs = retriever.get_relevant_documents(query)

    print("\nTop K Document Paths:")
    for i, doc in enumerate(docs, 1):
        path = doc.metadata.get("source", "Unknown Path")  # Assuming 'source' contains the file path
        print(f"{i}. {path}")

# Run an example query
if __name__ == "__main__":
    user_query = input("Enter your query: ")
    search_query(user_query, k=5)
# Evaluation and Observations
# 1. The use of HuggingFaceEmbeddings allows for effective semantic representation of text, enhancing the accuracy of search results.
# 2. The choice of the VietAI vit5-base model for text generation is suitable for Vietnamese language tasks, potentially improving the relevance of responses.
# 3. Setting the parameter 'k' to 5 limits the number of retrieved documents, which can help in managing the output and focusing on the most relevant results.